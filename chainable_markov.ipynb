{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainable Markov Chain Model\n",
    "\n",
    "Trains a model that learns a chainable composition operation in latent space for Markov chain prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharVocab:\n",
    "    \"\"\"Character-level tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        chars = sorted(set(text))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [self.char_to_idx[ch] for ch in text]\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return ''.join(self.idx_to_char[i] for i in indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramDataset(Dataset):\n",
    "    \"\"\"Creates 2-5 gram samples from text.\"\"\"\n",
    "\n",
    "    def __init__(self, text: str, vocab: CharVocab, min_n: int = 2, max_n: int = 5):\n",
    "        self.vocab = vocab\n",
    "        self.min_n = min_n\n",
    "        self.max_n = max_n\n",
    "\n",
    "        # Encode full text\n",
    "        self.encoded = vocab.encode(text)\n",
    "\n",
    "        # Create all n-gram samples\n",
    "        self.samples = []\n",
    "        for n in range(min_n, max_n + 1):\n",
    "            for i in range(len(self.encoded) - n):\n",
    "                # Input: first n tokens, Target: (n+1)th token\n",
    "                context = self.encoded[i:i+n]\n",
    "                target = self.encoded[i+n]\n",
    "                self.samples.append((context, target))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[list[int], int]:\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "def collate_ngrams(batch: list[tuple[list[int], int]]) -> tuple[list[list[int]], torch.Tensor]:\n",
    "    \"\"\"Collate variable-length n-grams.\"\"\"\n",
    "    contexts = [item[0] for item in batch]\n",
    "    targets = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    return contexts, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainable Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainableMarkovModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that learns a chainable composition operation in latent space.\n",
    "\n",
    "    Key property: composition outputs live in the SAME space as embeddings,\n",
    "    enabling arbitrary-length chaining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_latent: int = 128,\n",
    "        d_hidden: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_latent = d_latent\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Token embedding: token_id -> latent_vector\n",
    "        self.embedding = nn.Embedding(vocab_size, d_latent)\n",
    "\n",
    "        # Composition MLP: (latent, latent) -> latent (CHAINABLE)\n",
    "        self.compose_mlp = nn.Sequential(\n",
    "            nn.Linear(d_latent * 2, d_hidden),\n",
    "            nn.LayerNorm(d_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            nn.LayerNorm(d_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_hidden, d_latent),\n",
    "        )\n",
    "\n",
    "        # Decoder head: latent -> logits\n",
    "        # Structure: latent -> hidden -> latent -> vocab (for weight tying)\n",
    "        self.decoder_hidden = nn.Sequential(\n",
    "            nn.Linear(d_latent, d_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_hidden, d_latent),\n",
    "        )\n",
    "        # Output projection with weight tying to embedding\n",
    "        self.decoder_out = nn.Linear(d_latent, vocab_size, bias=False)\n",
    "        self.decoder_out.weight = self.embedding.weight  # Weight tying\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def embed(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embed tokens into latent space.\"\"\"\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "    def compose_latents(self, latent1: torch.Tensor, latent2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compose two latent vectors into one.\n",
    "\n",
    "        CHAINABLE: Output lives in same 128-dim space as inputs.\n",
    "        \"\"\"\n",
    "        combined = torch.cat([latent1, latent2], dim=-1)\n",
    "        return self.compose_mlp(combined)\n",
    "\n",
    "    def decode(self, latent: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode latent to vocabulary logits.\"\"\"\n",
    "        hidden = self.decoder_hidden(latent)\n",
    "        return self.decoder_out(hidden)\n",
    "\n",
    "    def forward_chain(self, token_sequences: list[list[int]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Process variable-length token sequences through left-to-right composition.\n",
    "\n",
    "        For tokens [t1, t2, t3, t4]:\n",
    "            result = compose(compose(compose(embed(t1), embed(t2)), embed(t3)), embed(t4))\n",
    "        \"\"\"\n",
    "        batch_latents = []\n",
    "\n",
    "        for seq in token_sequences:\n",
    "            # Convert to tensor and embed all tokens\n",
    "            tokens = torch.tensor(seq, dtype=torch.long, device=device)\n",
    "            embeddings = self.embed(tokens)  # (seq_len, d_latent)\n",
    "\n",
    "            # Left-to-right composition\n",
    "            latent = embeddings[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                latent = self.compose_latents(latent, embeddings[i])\n",
    "\n",
    "            batch_latents.append(latent)\n",
    "\n",
    "        return torch.stack(batch_latents)  # (batch_size, d_latent)\n",
    "\n",
    "    def forward(self, token_sequences: list[list[int]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Full forward pass: compose tokens and decode to logits.\"\"\"\n",
    "        latent = self.forward_chain(token_sequences, device)\n",
    "        return self.decode(latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: ChainableMarkovModel,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 3e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    gradient_clip: float = 1.0,\n",
    ") -> ChainableMarkovModel:\n",
    "    \"\"\"Training loop with cosine annealing LR schedule.\"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_steps = 0\n",
    "\n",
    "        for contexts, targets in train_loader:\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(contexts, device)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for contexts, targets in val_loader:\n",
    "                targets = targets.to(device)\n",
    "                logits = model(contexts, device)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_steps\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'markov_model_best.pt')\n",
    "\n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('markov_model_best.pt', weights_only=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: ChainableMarkovModel,\n",
    "    vocab: CharVocab,\n",
    "    seed: str,\n",
    "    max_length: int = 200,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.9,\n",
    "    device: torch.device = torch.device('cpu'),\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text from a seed string using temperature and nucleus sampling.\n",
    "\n",
    "    1. Embed and compose all seed characters\n",
    "    2. Decode latent -> sample next token\n",
    "    3. Compose new token with current latent\n",
    "    4. Repeat for desired length\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode seed and get initial latent\n",
    "    tokens = vocab.encode(seed)\n",
    "\n",
    "    # Initial composition of seed\n",
    "    embeddings = model.embed(torch.tensor(tokens, dtype=torch.long, device=device))\n",
    "    latent = embeddings[0]\n",
    "    for i in range(1, len(tokens)):\n",
    "        latent = model.compose_latents(latent, embeddings[i])\n",
    "\n",
    "    generated = list(seed)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Decode to logits\n",
    "        logits = model.decode(latent)\n",
    "\n",
    "        # Temperature scaling\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Nucleus (top-p) sampling\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # Remove tokens above threshold\n",
    "        sorted_indices_to_remove = cumsum > top_p\n",
    "        sorted_indices_to_remove[0] = False  # Keep at least one token\n",
    "        sorted_probs[sorted_indices_to_remove] = 0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "        # Sample\n",
    "        idx = torch.multinomial(sorted_probs, 1)\n",
    "        next_token = sorted_indices[idx].item()\n",
    "\n",
    "        generated.append(vocab.idx_to_char[next_token])\n",
    "\n",
    "        # Compose new token with current latent for next iteration\n",
    "        next_embedding = model.embed(torch.tensor([next_token], dtype=torch.long, device=device))[0]\n",
    "        latent = model.compose_latents(latent, next_embedding)\n",
    "\n",
    "    return ''.join(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Tiny Shakespeare\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filepath = 'shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Downloading from {url}...\")\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Corpus size: {len(text):,} characters\")\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = CharVocab(text)\n",
    "print(f\"Vocabulary size: {vocab.vocab_size}\")\n",
    "\n",
    "# Train/validation split\n",
    "split_idx = int(len(text) * 0.9)\n",
    "train_text = text[:split_idx]\n",
    "val_text = text[split_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NgramDataset(train_text, vocab)\n",
    "val_dataset = NgramDataset(val_text, vocab)\n",
    "print(f\"Train samples: {len(train_dataset):,}\")\n",
    "print(f\"Val samples: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "d_latent = 128\n",
    "d_hidden = 512\n",
    "dropout = 0.1\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "lr = 3e-4\n",
    "weight_decay = 0.01\n",
    "gradient_clip = 1.0\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_ngrams,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_ngrams,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = ChainableMarkovModel(\n",
    "    vocab_size=vocab.vocab_size,\n",
    "    d_latent=d_latent,\n",
    "    d_hidden=d_hidden,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    gradient_clip=gradient_clip,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_seqs = [\n",
    "        vocab.encode(\"ab\"),      # 2-gram\n",
    "        vocab.encode(\"abc\"),     # 3-gram\n",
    "        vocab.encode(\"abcd\"),    # 4-gram\n",
    "        vocab.encode(\"abcde\"),   # 5-gram\n",
    "    ]\n",
    "    print(\"Chainability Test:\")\n",
    "    for seq in test_seqs:\n",
    "        latent = model.forward_chain([seq], device)\n",
    "        print(f\"  {len(seq)}-gram: latent shape = {latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"Follow those\"\n",
    "print(f\"Seed: '{seed}'\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "generated = generate(\n",
    "    model,\n",
    "    vocab,\n",
    "    seed,\n",
    "    max_length=200,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    device=device,\n",
    ")\n",
    "print(generated)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_char_to_idx': vocab.char_to_idx,\n",
    "    'vocab_idx_to_char': vocab.idx_to_char,\n",
    "    'd_latent': d_latent,\n",
    "    'd_hidden': d_hidden,\n",
    "}, 'markov_model_final.pt')\n",
    "print(\"Model saved to markov_model_final.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
