{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainable Markov Chain Model\n",
    "\n",
    "Trains a model that learns a chainable composition operation in latent space for Markov chain prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Mount Drive and Clone Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Google Drive mounting (commented out - using local storage instead)\n# from google.colab import drive\n# drive.mount('/content/drive')\n\n# Use local output directory (persists only during runtime)\nimport os\nOUTPUT_DIR = '/content/out'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Outputs will be saved to: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sughodke/markov-learned.git\n",
    "%cd markov-learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import (\n",
    "    CharVocab,\n",
    "    NgramDataset,\n",
    "    ChainableMarkovModel,\n",
    "    collate_ngrams,\n",
    "    train,\n",
    "    generate,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import wandb\nfrom google.colab import userdata\n\n# Login with API key from Colab secrets\nwandb.login(key=userdata.get('WANDB_API_KEY'))\n\n# Hyperparameters\nconfig = {\n    'd_latent': 128,\n    'd_hidden': 512,\n    'dropout': 0.1,\n    'batch_size': 256,  # Increased for faster training\n    'epochs': 10,       # Reduced from 50\n    'lr': 3e-4,\n    'weight_decay': 0.01,\n    'gradient_clip': 1.0,\n}\n\nwandb.init(\n    project='chainable-markov',\n    config=config,\n    name='shakespeare-run-v3',\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Corpus size: {len(text):,} characters\")\n",
    "\n",
    "vocab = CharVocab(text)\n",
    "print(f\"Vocabulary size: {vocab.vocab_size}\")\n",
    "\n",
    "# Train/validation split\n",
    "split_idx = int(len(text) * 0.9)\n",
    "train_text = text[:split_idx]\n",
    "val_text = text[split_idx:]\n",
    "\n",
    "train_dataset = NgramDataset(train_text, vocab)\n",
    "val_dataset = NgramDataset(val_text, vocab)\n",
    "print(f\"Train samples: {len(train_dataset):,}\")\n",
    "print(f\"Val samples: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_ngrams, \n",
    "    num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_ngrams, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "model = ChainableMarkovModel(\n",
    "    vocab_size=vocab.vocab_size, \n",
    "    d_latent=config['d_latent'], \n",
    "    d_hidden=config['d_hidden'], \n",
    "    dropout=config['dropout']\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "wandb.config.update({'num_params': num_params})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model, history = train(\n    model, \n    train_loader, \n    val_loader, \n    device, \n    epochs=config['epochs'],\n    lr=config['lr'],\n    weight_decay=config['weight_decay'],\n    gradient_clip=config['gradient_clip'],\n    use_wandb=True,\n    vocab=vocab,  # Enable sample generation each epoch\n    sample_seed=\"The \",\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_path = f\"{OUTPUT_DIR}/markov_model_{timestamp}.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_char_to_idx': vocab.char_to_idx,\n",
    "    'vocab_idx_to_char': vocab.idx_to_char,\n",
    "    'd_latent': config['d_latent'],\n",
    "    'd_hidden': config['d_hidden'],\n",
    "    'history': history,\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "# Log model artifact to wandb\n",
    "artifact = wandb.Artifact('markov-model', type='model')\n",
    "artifact.add_file(save_path)\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"Follow those\"\n",
    "print(f\"Seed: '{seed}'\")\n",
    "print(\"-\" * 40)\n",
    "generated_text = generate(model, vocab, seed, max_length=200, temperature=0.8, device=device)\n",
    "print(generated_text)\n",
    "\n",
    "# Log generated text to wandb\n",
    "wandb.log({'generated_text': wandb.Html(f'<pre>{generated_text}</pre>')})\n",
    "\n",
    "# Save generated text\n",
    "with open(f\"{OUTPUT_DIR}/generated_{timestamp}.txt\", 'w') as f:\n",
    "    f.write(generated_text)\n",
    "print(f\"\\nGenerated text saved to Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for n in [2, 3, 4, 5]:\n",
    "        seq = vocab.encode(\"a\" * n)\n",
    "        latent = model.forward_chain([seq], device)\n",
    "        print(f\"{n}-gram: latent shape = {latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish W&B Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model (for future sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify path to load a previously saved model\n",
    "# checkpoint = torch.load(f\"{OUTPUT_DIR}/markov_model_YYYYMMDD_HHMMSS.pt\")\n",
    "# \n",
    "# vocab = CharVocab.__new__(CharVocab)\n",
    "# vocab.char_to_idx = checkpoint['vocab_char_to_idx']\n",
    "# vocab.idx_to_char = checkpoint['vocab_idx_to_char']\n",
    "# vocab.vocab_size = len(vocab.char_to_idx)\n",
    "# \n",
    "# model = ChainableMarkovModel(\n",
    "#     vocab_size=vocab.vocab_size,\n",
    "#     d_latent=checkpoint['d_latent'],\n",
    "#     d_hidden=checkpoint['d_hidden'],\n",
    "# )\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model = model.to(device)\n",
    "# print(\"Model loaded!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}